# ADCIRC Use Case - Using Tapis and Pylauncher for Ensemble Modeling in DesignSafe 

**Clint Dawson, University of Texas at Austin**  
**Carlos del-Castillo-Negrete, University of Texas at Austin**  
**Benjamin Pachev, University of Texas at Austin**  

The following use case presents an example of how to leverage the Tapis API to run an ensemble of HPC simulations. The specific workflow to be presented consists of running ADCIRC, a storm-surge modeling application available on DesignSafe, using the parametric job launcher pylauncher. All code and examples presented are meant to be be executed from a Jupyter Notebook on the DesignSafe platform and using a DesignSafe account to make Tapis API calls. Accompanying jupyter notebooks for this use case can be found in the ADCIRC folder in [Community Data](https://www.designsafe-ci.org/data/browser/public/designsafe.storage.community/Use%20Case%20Products/ADCIRC).

Learn more: [Jupyter notebooks on DS Juypterhub](https://www.designsafe-ci.org/rw/workspace/#!/Jupyter::Analysis).

## Background 

### Citation and Licensing

* Please cite [Rathje et al. (2017)](https://doi.org/10.1061/(ASCE)NH.1527-6996.0000246) to acknowledge the use of DesignSafe resources.  

* This software is distributed under the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html).  

### ADCIRC

For more information on running ADCIRC and documentation, see the following links:

* [ADCIRC Wiki](https://wiki.adcirc.org/wiki/Main_Page)
* [ADCIRC Web Page](https://adcirc.org/)

ADCIRC is available as a standalone app accesible via the [DesignSafe front-end](https://www.designsafe-ci.org/rw/workspace/#!/ADCIRC::Simulation).

ADCIRC files used in this demo are pre-staged on TACC resources that DesignSafe execution systems have access to, at the path `/work/06307/clos21/pub/adcirc`. See the section on using data from Projects directory for using other data sources.

### Tapis

Tapis is the main API to control and access HPC resources with. For more resources and tutorials on how to use Tapis, see the following:

* [Tapis CLI](https://tapis-cli.readthedocs.io/en/latest/contents.html)
* [AgavePy](https://tacc-cloud.readthedocs.io/projects/agave/en/latest/)
* [DesignSafe Webinar](https://www.youtube.com/watch?v=-_1lNWW8CAg&t=1854s&ab_channel=NHERIDesignSafe-CIMedia)

To initialize tapis in our jupyter notebook we use AgavePy. Relies on `tapis auth init --interactive` being run from a terminal first upon initializing your Jupyter server.

![caption](img/TapisImage.png)
> Initialize Tapis from within a shell in a jupyter session. A shell can be launched by going to File -> New -> Terminal.

Once this is complete, you can now run from a code cell in your jupyter session the following to load your AgavePy credentials:

```python
from agavepy.agave import Agave

ag = Agave.restore()
```

### Pylauncher

[Pylauncher](https://github.com/TACC/pylauncher) is a parametric job launcher used for launching a collection of HPC jobs within one HPC job. By specifying a list of jobs to execute in either a CSV or json file, pylauncher manages resources on a given HPC job to execute all the jobs using the given nodes. Inputs for pylauncher look something like (for csv files, per line):

```
num_processes,<pre process command>;<main parallel command>;<post process command>
```

The pre-process and post-process commands are executed in serial, while the main command is executed in parallel using the appropriate number of processes. Note pre and post process commands should do light file management and movement and no computationally intensive tasks.

## Tapis Pylauncher App


Overview of this section:

* Getting the Appication
* App Overview
* Staging Files
* Example Ensemble ADCIRC RUN
  * Configuring
  * Submitting and Monitoring
  * Getting and visualizing output

### Accessing the Application

Check out the application from the github page - [https://github.com/UT-CHG/tapis-pylauncher](https://github.com/UT-CHG/tapis-pylauncher) and deploy it using either the Tapis CLI or AgavePy (See documentation links above under Tapis section), or email cdelcastillo21@gmail.com for access to an already deployed version of the application (it is not a public application yet, so has to be shared explicitly with users).

### Basic Application Overview

The tapis-pylauncher application loops through iterations of calling pylauncher utility, using as input a file generated by a user defined generator shell script `generator.sh`. An simplified excerpt of this main execution loop is as follows:

```bash
# Main Execution Loop:
#   - Call generator script.
#   - Calls pylauncher on generated input file. Expected name = jobs_list.csv
#   - Repeats until generator script returns no input file for pylauncher.
ITER=1
while :
do
  # Call generator if it exists script
  if [ -e generator.sh ]
  then
    
    ./generator.sh ${ITER} $SLURM_NPROCS $generator_args
  fi

  # If input file for pylauncher has been generated, then start pylauncher
  if [ -e ${pylauncher_input} ]
  then
    python3 launch.py ${pylauncher_input} >> pylauncher.log
  fi

  ITER=$(( $ITER + 1 ))
done
```

Note how a generator script is not required, with a static pylauncher file, of input name determined as a job parameter `pylauncher_input`, being sufficient to run a single batch of jobs. 

All input scripts and files for each parametric job should be zipped into a file and passed as an input to the pylauncher application. Note that these files shouldn't be too large and shouldn't contain data as tapis will be copying them around to stage and archive jobs. Data should ideally be pre-staged and not part of the zipped job inputs.

### Staging Files 

For large scale ensemble simulations, it is best to stage individual ADCIRC run files in a project directory that execution systems can access before-hand so that Tapis itself isn't doing the moving and staging of data. 

The corresponding TACC base path to your project with a particular id can be found at `/corral-repl/projects/NHERI/projects/[id]/`. To find the ID for your project, you can just look at the URL of your project directory in designsafe:

![caption](img/project_dir.png)
> TX FEMA storms project directory. Note how the URL on top contains the Project ID corresponding to the path on corral that execution systems on tapis can access.

### Example Ensemble Run: Shinnecock Inlet Test Grid Performance

This section contains info about how to run a simple ADCIRC run using the pylauncher app. This example has an accompanying notebook in the [ADCIRC Use Case folder](https://www.designsafe-ci.org/data/browser/public/designsafe.storage.community/Use%20Case%20Products/ADCIRC) in the Community Data directory, with more depth in discussion and details.

We will run ADCIRC on the [Shinnecock Inlet Test Grid](https://adcirc.org/home/documentation/example-problems/shinnecock-inlet-ny-with-tidal-forcing-example/) and run an ensemble of simulations using a differing amount of parallel processes on the same simulation. Our goal is to see how the performance of this test grid scales with the number of processors used. 

![caption](img/si_mesh.png)
> Shinnecock Inlet Test Grid. ADCIRC solves the Shallow Water Equations over a Triangular Mesh, depicted above. On the right we see one of the stations, station 2, we will be analyzing output for.

#### Staging Inputs

The job inputs for the simple adcirc parametric run is available in the job_configs/adcirc directory of the [ADCIRC UseCase directory](https://www.designsafe-ci.org/data/browser/public/designsafe.storage.community/Use%20Case%20Products/ADCIRC/job_configs/adcirc).

* setup.sh - Setup script to run before running any ensemble jobs. Sets up runs and logs directory, which is redundant in this case since we are only running a singular run.
* post_process.sh - Script to run per-job to set-up each ADCIRC run using adcprep.
* pre_process.sh - Script to run per-job after each ADCIRC run to move outputs and logs to appropriate directories and cleanup.
* jobs_list.json - Generated within the notebook, this json file contains the ensemble jobs to be managed by pylauncher.

This directory of job input scripts and configurations is zipped before sent as an input parameter to tapis.

#### Configuring and Submitting Job

For configuring an individual run, we must build a job configuration dictionary first:

```python
uid = os.environ['JUPYTERHUB_USER']
notebook_path = f'{uid}/ADCIRC-UseCase'
notebook_uri = input_uri = f'agave://designsafe.storage.default/{notebook_path}'

simple_config = {}
simple_config['name'] = 'simple_example'
simple_config['appId'] =  py_app_id
simple_config['nodeCount'] = 1
simple_config['processorsPerNode'] =  10
simple_config['memoryPerNode'] = '1'
simple_config['maxRunTime'] = '00:30:00'
simple_config['archive'] = True
simple_config['archiveOnAppError'] = True
simple_config['inputs'] = {'job_inputs': input_uri}
simple_config['parameters'] = {'pylauncher_input': 'jobs_list.json'}
```

Note how the `job_inputs` is the path to the inputs zip file on Tapis systems, which has to be preceded by an agave URI that Tapis uses to locate the file. For more info on AgavePy file naming conventions and storage systems see the the documentation in the tapis links above.

#### Monitoring Job 

We can get our jobs status by using the `getStatus` command. 

```python
ag.jobs.getStatus(jobId=job['id'])
```

Note we must wait for it to reach a `FINISHED` state, after archiving, to download outputs.


```python
{'id': 'e64c31e9-598e-4c52-9e29-5d36292fa1a3-007',
 'status': 'FINISHED',
 '_links': {'self': {'href': 'https://agave.designsafe-ci.org/jobs/v2/e64c31e9-598e-4c52-9e29-5d36292fa1a3-007'}}}
```

But we can look at the job directory files as they execute to monitor how the job is doing. For example, we can look at the runs directory to see what runs have been started:

```python
[f for f in ag.jobs.listOutputs(filePath='runs', jobId=job['id'])]
```

We should see a list of dictionaries such as:

```python
[{'name': 'job_1',
  'path': 'runs/job_1',
  'lastModified': datetime.datetime(2022, 3, 15, 18, 54, 50, tzinfo=tzlocal()),
  'length': 4096,
  'owner': '?',
  'permission': 'READ_WRITE',
  'mimeType': 'text/directory',
  'format': 'folder',
  'type': 'dir',
  '_links': {'self': {'href': 'https://agave.designsafe-ci.org/jobs/v2/12a21b19-4b5b-4fbc-bc0a-dc25b0d7367f-007/outputs/media/runs/job_1'},
   'system': {'href': 'https://agave.designsafe-ci.org/systems/v2/designsafe.storage.default'},
   'parent': {'href': 'https://agave.designsafe-ci.org/jobs/v2/12a21b19-4b5b-4fbc-bc0a-dc25b0d7367f-007'}}}]
```

#### Getting Job Output

Once the job reaches the archived state, we can see the archive path of the job, which should be accessible from our notebook.

```pyhton
job = ag.jobs.get(jobId=job['id'])
job_archive_path = Path(prefix) / Path(job['archivePath']).relative_to('clos21')
```

For interacting with ADCIRC output data we will primarily use the xarray library:

```python
import xarray as xa

f61 = xa.load_dataset(job_archive_path / 'outputs' / 'job_1.61.nc')
f61
```

We should see an xarray structure looking something like:

![caption](img/f61_xarray.png)
> Example ADCIRC time-series xarray data-structure

We can quickly plot using xarray's native plotting capabilities:

```python
f61.isel(station=2)['zeta'].plot()
```

![caption](img/si_tides_ts.png)
> Example ADCIRC time-series output, this example contains tide only forcing, so we see a pretty basic periodic signal.

#### Analyzing Logs

Logs for the main pylauncher thread that manages all the jobs can be found in the root directory of the job, ending in `.out`. It has a nice structure that can be loaded and read by by the pandas library to analyze logs and see how your job is doing:

```
log_file = [f.name for f in (job_archive_path).iterdir() if f.name.endswith('out')][0]
log = pd.read_csv(job_archive_path / log_file, delimiter='|')
log
```
![caption](img/logs_full.png)
> Example Log file for full simulation run.

Log files for each job are stored in the `logs/` directory of any job, and are saved per job. They can also be loaded conveniently using pandas:

```python
log_path = job_archive_path / 'logs/job_1_GD-WindMult_WindJan2018_CFSv2_12.log'

log = pd.read_csv(log_path, delimiter='|')
log
```

We should see a pandas data-frame with something like:

![caption](img/logs_job.png)
> Example Log file for an individual simulation.
